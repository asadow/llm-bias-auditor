# LLM Backend Configuration
LLM_BACKEND=ollama

# Ollama Configuration (default)
# Tip: On 8GB Macs, prefer smaller models such as:
#   - llama3.2:3b
#   - qwen2.5:1.5b
#   - phi3:mini
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# OpenAI Configuration (optional - only needed if LLM_BACKEND=openai)
# Set these to offload inference to a cloud API instead of running locally
# OPENAI_API_KEY=your-api-key-here
# OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_MODEL=gpt-4o-mini
